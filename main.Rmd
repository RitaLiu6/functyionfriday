---
title: "TidyText"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      error = TRUE)
```



## Resources

* [Introduction to tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) by Julia Silge and David Robinson. We get a lot of help from this document. 
* [Text Mining with R](https://www.tidytextmining.com/index.html)  by Julia Silge and David Robinson. This is comprehensive overview about how to deal with text in R. 
* [Package ‘tidytext’](https://cran.r-project.org/web/packages/tidytext/tidytext.pdf). An official document listing all the functions in tidytext and their usage.

## Set Up: 
```{r}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
```

Here, we will use `austen_books()` to load Austen's 6 completed, published novels, mutate the dataset to obtain the chapter and line of each line. 
```{r}
# load materials
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(line = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
original_books
```

## What is Tidytext and why is it useful?
Tidytext is a package that allows for easier text analysis and gives analysts tools needed for text mining tools that can be used in conjunction with packages we already know, such as dplyr and ggplot2. The package makes the manipulation, representation, and visualization of text-driven datasets and analyses easier to manage. Tidytext also features sentiment analysis, which can help analysts draw large, general conclusions regarding the content of a given text through the creation of visualizations and through calculating numerical summaries in the realm of textual analysis such as a list of the most used words in a given text. Tidytext is essentially an extremely useful supplementary tool to be used in conjunction with packages we already know.

## Use unnest_tokens() to process the data frame
`unnest_tokens()` in tidytext helps us to split a column into shorter phrases of our choice. The arguments are as following:  
* output: the name for the output column   
* input: the name of input column that gets split  
* token: how we want to split the column. Default is "words". Other options are "characters", "character_shingles", "ngrams", "skip_ngrams", "sentences", "lines", "paragraphs", "regex", "tweets" . 

```{r}
tidy_books <- original_books %>%
  unnest_tokens(word, text)
# token = 'regex', pattern = "Chapter|CHAPTER [\\dIVXLC]"
tidy_books
```


# stop_words and interaction with dplyr
```{r}
cleaned_books <- tidy_books %>%
  anti_join(stop_words %>% filter(lexicon == "SMART"),
            by = 'word')
cleaned_books
```

```{r}
#loading required packages
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>% #using unnest_tokens to turn sentences from each book into a long list of words for analysis, there are a total of 725,055 rows in this dataset.
    anti_join(stop_words, by = "word") %>% #using dplyr to get rid of stop words (explain dpyr)
  count(book, word, sort = TRUE) #counting the occurrence of each word and sorting them by how often they occur. 

book_words
```


## use bind_tf_idf to get term frequency and importance
`use_bind_tf_idf`can be used to find term frequency and inverse document frequency of a tidy text dataset.Term frequency shows the frequency the word is used in books provided, weighted by the length of the book. Inverse term frequency indicates number of documents in the corpus that contain the word. The multiply of tf and idf is often used to show how important a word is to a document in a collection. For more information about tf and idf, you can look at [this](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).   
The arguments are following: 
* term: the column name that contains the term  
* document: column name that contains name or ID of the document  
* n: column name that contains the counts of the term

```{r}
tidy_books %>% 
  count(book, word, sort = TRUE) %>% 
  anti_join(stop_words) %>% 
  bind_tf_idf(word, book, n) %>%
  arrange(desc(tf_idf))
```


```{r}
# add an example of a viz involving get_sentiments
```

## get_sentiments() 

<br> The get_sentiments is a function that allows us to get specific sentiment lexicons with appropriate categories for the words. The three general purpose lexicons are AFINN, bing, nrc: 

  # AFINN - a lexicon measuring sentiment with a numeric score between -5 and +5
  # Bing - A binary approach that divides words into either "positive" or "negative"
  # NRC - categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 

```{r}
positive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive") # This filters the entire dataset to look for unigrams (single words) that have positive connotations
positive
```
```{r}
cleaned_books %>% 
  semi_join(positive, by = "word") %>%
  count(word, sort = TRUE) %>%
  slice(0:15) %>% 
  ggplot(aes(x = n , y = reorder(word,n))) +
  geom_col()
  
  
```


# Application Uses 
Think about scenarios that we can use tidytext to process information? Why it is useful? Can you think of limitation of it? 

