---
title: "TidyText"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Resource:  

## Setting Up R: 
```{r}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
```

```{r}
# load materials
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(line = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
original_books
```

## Introduction
What is tidytext and why is it useful?
## unnest_tokens()
```{r}
tidy_books <- original_books %>%
  filter(book == 'Sense & Sensibility') %>% 
  unnest_tokens(word, text, token = "ngrams", n = 3)

tidy_books
```

## get_sentiments() 

<br> The get_sentiments is a function that allows us to get specific sentiment lexicons with appropriate categories for the words. The three general purpose lexicons are AFINN, bing, nrc: 

  # AFINN - a lexicon measuring sentiment with a numeric score between -5 and +5
  # Bing - A binary approach that divides words into either "positive" or "negative"
  # NRC - categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 

```{r}
positive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive") # This filters the entire dataset to look for unigrams (single words) that have positive connotations
positive
```
```{r}
cleaned_books %>% 
  semi_join(positive, by = "word") %>%
  count(word, sort = TRUE) %>%
  slice(0:15) %>% 
  ggplot(aes(x = n , y = reorder(word,n))) +
  geom_col()
  
  
```

# stop_words and interaction with dplyr
```{r}
cleaned_books <- tidy_books %>%
  anti_join(stop_words %>% filter(lexicon == "SMART"),
            by = 'word')
cleaned_books
```

```{r}
#loading required packages
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>% #using unnest_tokens to turn sentences from each book into a long list of words for analysis, there are a total of 725,055 rows in this dataset.
    anti_join(stop_words, by = "word") %>% #using dplyr ro get rid of stop words (explain dpyr)
  count(book, word, sort = TRUE) #counting the occurance of each word and sorting them by how often they occur. 

book_words
```


# Application Uses 


