---
title: "TidyText"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Resources

* [Introduction to tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) by Julia Silge and David Robinson. We get a lot of help from this document. 
* [Text Mining with R](https://www.tidytextmining.com/index.html)  by Julia Silge and David Robinson. This is comprehensive overview about how to deal with text in R. 
* [Package ‘tidytext’](https://cran.r-project.org/web/packages/tidytext/tidytext.pdf). An official document listing all the functions in tidytext and their usage.

## Set Up: 
```{r}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
```

Here, we will use `austen_books()` to load Austen's 6 completed, published novels, mutate the dataset to obtain the chapter and line of each line. 
```{r}
# load materials
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(line = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()
original_books
```

## What is Tidytext and why is it useful?
Tidytext is a package that allows for easier text analysis and gives analysts tools needed for text mining tools that can be used in conjunction with packages we already know, such as dplyr and ggplot2. The package makes the manipulation, representation, and visualization of text-driven datasets and analyses easier to manage. Tidytext also features sentiment analysis, which can help analysts draw large, general conclusions regarding the content of a given text through the creation of visualizations and through calculating numerical summaries in the realm of textual analysis such as a list of the most used words in a given text. Tidytext is essentially an extremely useful supplementary tool to be used in conjunction with packages we already know.

## Use unnest_tokens() to process the data frame
`unnest_tokens()` in tidytext helps us to split a column into shorter phrases of our choice. The arguments are as following:  
* output: the name for the output column   
* input: the name of input column that gets split  
* token: how we want to split the column. Default is "words". Other options are "characters", "character_shingles", "ngrams", "skip_ngrams", "sentences", "lines", "paragraphs", "regex", "tweets" . 

```{r}
tidy_books <- original_books %>%
  filter(book == 'Sense & Sensibility') %>% 
  unnest_tokens(word, text)
# token = "regex",  pattern = "Chapter [\\\\d]" ?? How to use regex
tidy_books
```
```{r}
tidy_books %>% 
  count(book, word, sort = TRUE) %>% 
  anti_join(stop_words) %>% 
  bind_tf_idf(word, book, n) %>%
  arrange(desc(tf))
```


# stop_words and interaction with dplyr
```{r}
cleaned_books <- tidy_books %>%
  anti_join(stop_words %>% filter(lexicon == "SMART"),
            by = 'word')
cleaned_books
```

```{r}
#loading required packages
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>% #using unnest_tokens to turn sentences from each book into a long list of words for analysis, there are a total of 725,055 rows in this dataset.
    anti_join(stop_words, by = "word") %>% #using dplyr to get rid of stop words (explain dpyr)
  count(book, word, sort = TRUE) #counting the occurrence of each word and sorting them by how often they occur. 

book_words
```

## get_sentiments() 

<br> The get_sentiments is a function that allows us to get specific sentiment lexicons with appropriate categories for the words. The three general purpose lexicons are AFINN, bing, nrc: 

  # AFINN - a lexicon measuring sentiment with a numeric score between -5 and +5
  # Bing - A binary approach that divides words into either "positive" or "negative"
  # NRC - categorizes words in a binary fashion ("yes"/"no") into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. 

```{r}
positive <- get_sentiments("bing") %>% 
  filter(sentiment == "positive") # This filters the entire dataset to look for unigrams (single words) that have positive connotations
positive
```
```{r}
cleaned_books %>% 
  semi_join(positive, by = "word") %>%
  count(word, sort = TRUE) %>%
  slice(0:15) %>% 
  ggplot(aes(x = n , y = reorder(word,n))) +
  geom_col()
  
  
```


# Application Uses 
Think about senarios that we can use tidytext to process information? Why it is useful? Can you think of limitation of it? 

